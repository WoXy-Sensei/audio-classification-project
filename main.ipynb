{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d0f3c8",
   "metadata": {},
   "source": [
    "### Step 1: We will prepare our dataset for analysis and extract sound signal features from  audio files using Mel-Frequency Cepstral Coefficients(MFCC)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9180a909",
   "metadata": {},
   "source": [
    "Every signal has its own characteristics. In sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound. Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC.\n",
    "\n",
    "You can get detailed info about MFC on : https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=0s\n",
    "\n",
    "So by using librosa library we will get characteristics of every audio signal in our dataset and hold them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d3b3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da3c9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d5b9f",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Here we will be using Mel-Frequency Cepstral Coefficients(MFCC) from the audio samples. The MFCC summarises the frequency distribution across the window size, so it is possible to analyse both the frequency and time characteristics of the sound. These audio representations will allow us to identify features for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \".\\dataset\"\n",
    "train = pd.read_csv(fr\"{dataset_path}\\train.csv\", index_col=[0])\n",
    "label_n = train['class'].nunique()\n",
    "labels = train['class'].unique()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efaa10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(filename):\n",
    "    audio, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=128)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e552e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features=[]\n",
    "for index, row in tqdm(train.iterrows()):\n",
    "   \n",
    "    file_name = rf\"{dataset_path}\\train2\\{row['filename']}\"\n",
    "    data = features_extractor(file_name)\n",
    "    extracted_features.append([data,row['class']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will convert extracted_features to Pandas dataframe and OneHotEncoder\n",
    "extracted_features_df = pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoder_df = pd.DataFrame(encoder.fit_transform(extracted_features_df[['class']]).toarray())\n",
    "\n",
    "encoder_df_list = encoder_df.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f07897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then split the dataset into independent and dependent dataset\n",
    "x = np.array(extracted_features_df['feature'].tolist())\n",
    "y = encoder_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extracted_features_df)\n",
    "print(encoder_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5af694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split dataset as Train and Test\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4be45",
   "metadata": {},
   "source": [
    "### Step 2: Building a Convolutional Neural Networks (CNN) Model and Train Our Model with UrbanSound8K Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many classes we have? We should  use it in ourm model\n",
    "num_labels = label_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6121a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start building our CNN model..\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1. hidden layer\n",
    "model.add(Dense(125,input_shape=(128,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# 2. hidden layer\n",
    "model.add(Dense(250))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# 3. hidden layer\n",
    "model.add(Dense(125))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Model özetini görüntüleme\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f68f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trianing the model\n",
    "epochscount = 200\n",
    "num_batch_size = 32\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=epochscount, validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f621887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8508771657943726\n"
     ]
    }
   ],
   "source": [
    "validation_test_set_accuracy = model.evaluate(x_test,y_test,verbose=0)\n",
    "print(\"Accuracy : \",validation_test_set_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e946b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/model_0.8508771657943726\\assets\n"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "model.save(f'./models/model_{validation_test_set_accuracy[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c3225",
   "metadata": {},
   "source": [
    "### Step 3: Finally We Predict an Audio File's Class Using Our CNN Model.\n",
    "\n",
    "We first preprocess the new audio data and then predict the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ba6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('path') # load model\n",
    "\n",
    "filename=rf\"path\"\n",
    "target_range = (-1, 1)\n",
    "y, sr = librosa.load(filename)\n",
    "y_normalized = (y - np.min(y)) / (np.max(y) - np.min(y)) * (target_range[1] - target_range[0]) + target_range[0]\n",
    "mfccs_features = librosa.feature.mfcc(y=y_normalized, sr=sr, n_mfcc=128)\n",
    "mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "mfccs_scaled_features = mfccs_scaled_features.reshape(1,-1)\n",
    "result_array = model.predict(mfccs_scaled_features)\n",
    "result = np.argmax(result_array[0])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
